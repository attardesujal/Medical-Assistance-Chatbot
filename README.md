ğŸ©º Aarogya â€“ Local Medical Chatbot
Aarogya is an intelligent medical assistant chatbot that runs completely offline using the Mistral 7B model via llama.cpp, with FAISS-based document search and multi-language support. Itâ€™s built with Streamlit, offers markdown-rich responses, and is optimized for GPU acceleration on local machines.

ğŸš€ Features
âœ… Runs entirely offline using a local .gguf Mistral model

âœ… FAISS vector store for fast, context-aware medical retrieval

âœ… ChatGPT-style markdown output (with headings, emojis, bullets)

âœ… Multi-language support via Google Translate

âœ… Streamlit UI with dark theme, logo, sidebar, and chat bubbles

âœ… Custom prompt tuning for precise medical Q&A

 Project Structure
Aarogya2/
â”œâ”€â”€ llama.cpp
â”œâ”€â”€ models/
â”‚   â””â”€â”€ mistral/
â”‚       â””â”€â”€ mistral-7b-instruct-v0.1.Q4_K_M.gguf
â”œâ”€â”€ vectorstore/
â”‚   â””â”€â”€ db_faiss/
â”‚       â”œâ”€â”€ index.faiss
â”‚       â””â”€â”€ index.pkl
â”‚                   
â””â”€â”€ app.py

ğŸ§  Model Download (Mistral 7B)
To run the chatbot locally, download the quantized GGUF model:

ğŸ“¥ [Mistral-7B-Instruct-v0.1.Q4_K_M.gguf - Hugging Face ](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF)

ğŸ”¸ Recommended: Q4_K_M version (~4GB)
ğŸ”¸ Place it in: models/mistral/mistral-7b-instruct-v0.1.Q4_K_M.gguf

llama-cpp-python handles the backend by default, but to build llama.cpp manually (for GPU tuning):
ğŸ“¥ggerganov/llama.cpp on GitHub

â–¶ï¸ Run the App
streamlit run app.py
